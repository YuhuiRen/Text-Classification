{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 6: Sentiment and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Unsupervised Sentiment Analysis\n",
    "\n",
    "In this question, you'll need the train dataset (`hw4_test.csv`) that you used in Assignment 4.\n",
    "\n",
    "\n",
    "Write a function `analyze_sentiment(docs, labels)` as follows:\n",
    "- Takes two inputs `docs` and `labels`, where `docs` are a list of documents, and `labels` are the ground-truth sentiment labels of `docs`\n",
    "- Use an unsupervised method (e.g. Vader, Textblob, ...) to determine the sentiment of each document in `docs`. Let's call the result `predicted_labels`\n",
    "- Calculate `AUC` using the ground truth `labels`\n",
    "- Return `predicted_labels`\n",
    "\n",
    "\n",
    "Next, compare `predicted_labels` with the predictions you get in HW 4. Answer the following questions:\n",
    "- Does the unsupervised method perform better (or worse) than the supervised method you took in HW 4?\n",
    "- Please explain why the unsupervised method is better (or worse) than the supervised one? Use one or more examples to illustrate your explanation. You can select a few samples where these two methods produce different predictions and investigate why one method fails or succeeds in these samples.\n",
    "\n",
    "\n",
    "Please write your analysis in a pdf document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We used the airbed for one guest (my very care...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I was introduced to Edgar Rice Burroughs when ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do not believe the charges of anti-semitism.Th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I tried to read this book when it first came o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I love KAL yeast flakes add this to your favor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  We used the airbed for one guest (my very care...      0\n",
       "1  I was introduced to Edgar Rice Burroughs when ...      1\n",
       "2  Do not believe the charges of anti-semitism.Th...      1\n",
       "3  I tried to read this book when it first came o...      0\n",
       "4  I love KAL yeast flakes add this to your favor...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"hw4_test.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(docs, labels):\n",
    "    \n",
    "    preds = None\n",
    "    \n",
    "    # Add your code here\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 80.32%\n"
     ]
    }
   ],
   "source": [
    "preds = analyze_sentiment(data[\"text\"], data[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Q2.** Topic Modeling\n",
    "\n",
    "In this question, you'll need the train and test datasets (`hw5_train.csv` and `hw5_test.csv`) that you used in Assignment 5.\n",
    "\n",
    "\n",
    "Write a function `analyze_topics(train_docs, test_docs, test_labels)` as follows:\n",
    "- This function takes the following parameters:\n",
    "    - `train_docs`: training documents\n",
    "    - `test_docs`: testing documents\n",
    "    - `test_labels`: the ground-truth labels of testing documents\n",
    "- Train an LDA model with 5 topics using the `train_docs`. Make sure you tune the parameters properly and train the model with sufficient iterations so that the model converges\n",
    "- Print top 20 words for each topic\n",
    "- Use the model to predict the topic mixture for each document in `test_docs`. \n",
    "- For each doc in `test_docs`, assign it the topic with the `maximum probability` of its topic mixture. Call the result `predicted_labels`\n",
    "- Map the topics to the ground-truth labels by majority vote\n",
    "- Calculate precision and recall of the predicted labels against the ground-truth labels\n",
    "- Return `predicted_labels`\n",
    "\n",
    "\n",
    "Next, compare `predicted_labels` with the predictions you get in HW 5. Are the predictions by LDA more (or less) accurate than the clustering methods you took in HW 5?\n",
    "\n",
    "\n",
    "Finally, so far we only picked one topic for each testing document. `How can we pick the most significant topics, which may be more than one, in a document?`\n",
    "\n",
    "\n",
    "Please write all your analyses into a pdf document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4 3 2 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bond game fails to shake or stir\\n\\nFor gaming...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student 'fee factor' played down\\n\\nA rise has...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Davenport hits out at Wimbledon\\n\\nWorld numbe...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weak dollar hits Reuters\\n\\nRevenues at media ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Johnson accuses British sprinters\\n\\nFormer Ol...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Bond game fails to shake or stir\\n\\nFor gaming...      5\n",
       "1  Student 'fee factor' played down\\n\\nA rise has...      4\n",
       "2  Davenport hits out at Wimbledon\\n\\nWorld numbe...      3\n",
       "3  Weak dollar hits Reuters\\n\\nRevenues at media ...      2\n",
       "4  Johnson accuses British sprinters\\n\\nFormer Ol...      3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv(\"hw5_train.csv\")\n",
    "test=pd.read_csv(\"hw5_test.csv\")\n",
    "\n",
    "print(test.label.unique())\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_topics(train_docs, test_docs, test_labels):\n",
    "\n",
    "    # add your code here\n",
    "    \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = analyze_topics(train[\"text\"], test[\"text\"], test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Q1\n",
      "AUC: 80.32%\n",
      "\n",
      "==================\n",
      "\n",
      "Test Q2\n",
      "iteration: 1 of max_iter: 200\n",
      "iteration: 2 of max_iter: 200\n",
      "iteration: 3 of max_iter: 200\n",
      "iteration: 4 of max_iter: 200\n",
      "iteration: 5 of max_iter: 200, perplexity: 2851.8392\n",
      "iteration: 6 of max_iter: 200\n",
      "iteration: 7 of max_iter: 200\n",
      "iteration: 8 of max_iter: 200\n",
      "iteration: 9 of max_iter: 200\n",
      "iteration: 10 of max_iter: 200, perplexity: 2685.3673\n",
      "iteration: 11 of max_iter: 200\n",
      "iteration: 12 of max_iter: 200\n",
      "iteration: 13 of max_iter: 200\n",
      "iteration: 14 of max_iter: 200\n",
      "iteration: 15 of max_iter: 200, perplexity: 2607.1235\n",
      "iteration: 16 of max_iter: 200\n",
      "iteration: 17 of max_iter: 200\n",
      "iteration: 18 of max_iter: 200\n",
      "iteration: 19 of max_iter: 200\n",
      "iteration: 20 of max_iter: 200, perplexity: 2571.1632\n",
      "iteration: 21 of max_iter: 200\n",
      "iteration: 22 of max_iter: 200\n",
      "iteration: 23 of max_iter: 200\n",
      "iteration: 24 of max_iter: 200\n",
      "iteration: 25 of max_iter: 200, perplexity: 2552.6132\n",
      "iteration: 26 of max_iter: 200\n",
      "iteration: 27 of max_iter: 200\n",
      "iteration: 28 of max_iter: 200\n",
      "iteration: 29 of max_iter: 200\n",
      "iteration: 30 of max_iter: 200, perplexity: 2537.3079\n",
      "iteration: 31 of max_iter: 200\n",
      "iteration: 32 of max_iter: 200\n",
      "iteration: 33 of max_iter: 200\n",
      "iteration: 34 of max_iter: 200\n",
      "iteration: 35 of max_iter: 200, perplexity: 2525.4604\n",
      "iteration: 36 of max_iter: 200\n",
      "iteration: 37 of max_iter: 200\n",
      "iteration: 38 of max_iter: 200\n",
      "iteration: 39 of max_iter: 200\n",
      "iteration: 40 of max_iter: 200, perplexity: 2514.5987\n",
      "iteration: 41 of max_iter: 200\n",
      "iteration: 42 of max_iter: 200\n",
      "iteration: 43 of max_iter: 200\n",
      "iteration: 44 of max_iter: 200\n",
      "iteration: 45 of max_iter: 200, perplexity: 2507.1254\n",
      "iteration: 46 of max_iter: 200\n",
      "iteration: 47 of max_iter: 200\n",
      "iteration: 48 of max_iter: 200\n",
      "iteration: 49 of max_iter: 200\n",
      "iteration: 50 of max_iter: 200, perplexity: 2496.4957\n",
      "iteration: 51 of max_iter: 200\n",
      "iteration: 52 of max_iter: 200\n",
      "iteration: 53 of max_iter: 200\n",
      "iteration: 54 of max_iter: 200\n",
      "iteration: 55 of max_iter: 200, perplexity: 2491.4599\n",
      "iteration: 56 of max_iter: 200\n",
      "iteration: 57 of max_iter: 200\n",
      "iteration: 58 of max_iter: 200\n",
      "iteration: 59 of max_iter: 200\n",
      "iteration: 60 of max_iter: 200, perplexity: 2486.6105\n",
      "iteration: 61 of max_iter: 200\n",
      "iteration: 62 of max_iter: 200\n",
      "iteration: 63 of max_iter: 200\n",
      "iteration: 64 of max_iter: 200\n",
      "iteration: 65 of max_iter: 200, perplexity: 2481.6388\n",
      "iteration: 66 of max_iter: 200\n",
      "iteration: 67 of max_iter: 200\n",
      "iteration: 68 of max_iter: 200\n",
      "iteration: 69 of max_iter: 200\n",
      "iteration: 70 of max_iter: 200, perplexity: 2476.7476\n",
      "iteration: 71 of max_iter: 200\n",
      "iteration: 72 of max_iter: 200\n",
      "iteration: 73 of max_iter: 200\n",
      "iteration: 74 of max_iter: 200\n",
      "iteration: 75 of max_iter: 200, perplexity: 2472.2934\n",
      "iteration: 76 of max_iter: 200\n",
      "iteration: 77 of max_iter: 200\n",
      "iteration: 78 of max_iter: 200\n",
      "iteration: 79 of max_iter: 200\n",
      "iteration: 80 of max_iter: 200, perplexity: 2468.6999\n",
      "iteration: 81 of max_iter: 200\n",
      "iteration: 82 of max_iter: 200\n",
      "iteration: 83 of max_iter: 200\n",
      "iteration: 84 of max_iter: 200\n",
      "iteration: 85 of max_iter: 200, perplexity: 2462.8343\n",
      "iteration: 86 of max_iter: 200\n",
      "iteration: 87 of max_iter: 200\n",
      "iteration: 88 of max_iter: 200\n",
      "iteration: 89 of max_iter: 200\n",
      "iteration: 90 of max_iter: 200, perplexity: 2455.6986\n",
      "iteration: 91 of max_iter: 200\n",
      "iteration: 92 of max_iter: 200\n",
      "iteration: 93 of max_iter: 200\n",
      "iteration: 94 of max_iter: 200\n",
      "iteration: 95 of max_iter: 200, perplexity: 2448.3057\n",
      "iteration: 96 of max_iter: 200\n",
      "iteration: 97 of max_iter: 200\n",
      "iteration: 98 of max_iter: 200\n",
      "iteration: 99 of max_iter: 200\n",
      "iteration: 100 of max_iter: 200, perplexity: 2442.8731\n",
      "iteration: 101 of max_iter: 200\n",
      "iteration: 102 of max_iter: 200\n",
      "iteration: 103 of max_iter: 200\n",
      "iteration: 104 of max_iter: 200\n",
      "iteration: 105 of max_iter: 200, perplexity: 2438.3002\n",
      "iteration: 106 of max_iter: 200\n",
      "iteration: 107 of max_iter: 200\n",
      "iteration: 108 of max_iter: 200\n",
      "iteration: 109 of max_iter: 200\n",
      "iteration: 110 of max_iter: 200, perplexity: 2432.8452\n",
      "iteration: 111 of max_iter: 200\n",
      "iteration: 112 of max_iter: 200\n",
      "iteration: 113 of max_iter: 200\n",
      "iteration: 114 of max_iter: 200\n",
      "iteration: 115 of max_iter: 200, perplexity: 2428.5561\n",
      "iteration: 116 of max_iter: 200\n",
      "iteration: 117 of max_iter: 200\n",
      "iteration: 118 of max_iter: 200\n",
      "iteration: 119 of max_iter: 200\n",
      "iteration: 120 of max_iter: 200, perplexity: 2426.6262\n",
      "iteration: 121 of max_iter: 200\n",
      "iteration: 122 of max_iter: 200\n",
      "iteration: 123 of max_iter: 200\n",
      "iteration: 124 of max_iter: 200\n",
      "iteration: 125 of max_iter: 200, perplexity: 2425.4829\n",
      "iteration: 126 of max_iter: 200\n",
      "iteration: 127 of max_iter: 200\n",
      "iteration: 128 of max_iter: 200\n",
      "iteration: 129 of max_iter: 200\n",
      "iteration: 130 of max_iter: 200, perplexity: 2424.5791\n",
      "iteration: 131 of max_iter: 200\n",
      "iteration: 132 of max_iter: 200\n",
      "iteration: 133 of max_iter: 200\n",
      "iteration: 134 of max_iter: 200\n",
      "iteration: 135 of max_iter: 200, perplexity: 2423.9539\n",
      "iteration: 136 of max_iter: 200\n",
      "iteration: 137 of max_iter: 200\n",
      "iteration: 138 of max_iter: 200\n",
      "iteration: 139 of max_iter: 200\n",
      "iteration: 140 of max_iter: 200, perplexity: 2423.5756\n",
      "iteration: 141 of max_iter: 200\n",
      "iteration: 142 of max_iter: 200\n",
      "iteration: 143 of max_iter: 200\n",
      "iteration: 144 of max_iter: 200\n",
      "iteration: 145 of max_iter: 200, perplexity: 2423.1676\n",
      "iteration: 146 of max_iter: 200\n",
      "iteration: 147 of max_iter: 200\n",
      "iteration: 148 of max_iter: 200\n",
      "iteration: 149 of max_iter: 200\n",
      "iteration: 150 of max_iter: 200, perplexity: 2422.7871\n",
      "iteration: 151 of max_iter: 200\n",
      "iteration: 152 of max_iter: 200\n",
      "iteration: 153 of max_iter: 200\n",
      "iteration: 154 of max_iter: 200\n",
      "iteration: 155 of max_iter: 200, perplexity: 2422.4587\n",
      "iteration: 156 of max_iter: 200\n",
      "iteration: 157 of max_iter: 200\n",
      "iteration: 158 of max_iter: 200\n",
      "iteration: 159 of max_iter: 200\n",
      "iteration: 160 of max_iter: 200, perplexity: 2422.0226\n",
      "iteration: 161 of max_iter: 200\n",
      "iteration: 162 of max_iter: 200\n",
      "iteration: 163 of max_iter: 200\n",
      "iteration: 164 of max_iter: 200\n",
      "iteration: 165 of max_iter: 200, perplexity: 2421.9020\n",
      "iteration: 166 of max_iter: 200\n",
      "iteration: 167 of max_iter: 200\n",
      "iteration: 168 of max_iter: 200\n",
      "iteration: 169 of max_iter: 200\n",
      "iteration: 170 of max_iter: 200, perplexity: 2421.6385\n",
      "iteration: 171 of max_iter: 200\n",
      "iteration: 172 of max_iter: 200\n",
      "iteration: 173 of max_iter: 200\n",
      "iteration: 174 of max_iter: 200\n",
      "iteration: 175 of max_iter: 200, perplexity: 2421.4389\n",
      "iteration: 176 of max_iter: 200\n",
      "iteration: 177 of max_iter: 200\n",
      "iteration: 178 of max_iter: 200\n",
      "iteration: 179 of max_iter: 200\n",
      "iteration: 180 of max_iter: 200, perplexity: 2421.3575\n",
      "Topic 0:\n",
      "[('said', '1316.37'), ('year', '678.62'), ('company', '398.39'), ('market', '393.11'), ('new', '338.90'), ('growth', '322.23'), ('economy', '311.97'), ('firm', '308.07'), ('2004', '302.34'), ('bank', '286.49'), ('china', '283.18'), ('sales', '272.73'), ('economic', '258.20'), ('world', '249.09'), ('oil', '238.20'), ('000', '233.49'), ('business', '230.63'), ('shares', '228.20'), ('uk', '217.74'), ('deal', '216.23')]\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "[('said', '920.28'), ('people', '725.91'), ('technology', '410.93'), ('mobile', '383.20'), ('use', '335.34'), ('users', '332.20'), ('games', '325.45'), ('net', '320.67'), ('digital', '319.19'), ('new', '315.28'), ('mr', '308.62'), ('software', '295.19'), ('music', '279.96'), ('online', '267.44'), ('like', '267.07'), ('phone', '251.28'), ('video', '247.65'), ('tv', '246.94'), ('make', '239.12'), ('service', '234.20')]\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "[('said', '635.29'), ('game', '474.18'), ('england', '385.76'), ('year', '359.50'), ('time', '346.68'), ('win', '329.24'), ('world', '319.81'), ('team', '277.99'), ('players', '258.31'), ('play', '257.29'), ('wales', '251.09'), ('ireland', '243.34'), ('good', '240.66'), ('just', '239.86'), ('cup', '233.20'), ('half', '232.27'), ('second', '225.56'), ('club', '222.81'), ('new', '221.66'), ('match', '220.91')]\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "[('film', '671.95'), ('best', '460.35'), ('said', '447.53'), ('year', '382.58'), ('music', '332.44'), ('new', '233.54'), ('awards', '221.20'), ('award', '218.19'), ('uk', '195.89'), ('star', '192.37'), ('won', '190.71'), ('director', '188.93'), ('tv', '161.59'), ('actor', '161.20'), ('number', '160.36'), ('british', '157.55'), ('films', '153.82'), ('years', '152.04'), ('band', '144.73'), ('including', '141.70')]\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "[('said', '2263.53'), ('mr', '1825.04'), ('government', '729.35'), ('people', '646.72'), ('labour', '631.19'), ('election', '518.58'), ('blair', '503.19'), ('party', '494.76'), ('new', '399.63'), ('minister', '396.31'), ('told', '393.88'), ('public', '360.38'), ('brown', '354.25'), ('say', '293.90'), ('says', '287.27'), ('plans', '271.26'), ('tax', '269.18'), ('bbc', '268.40'), ('howard', '263.89'), ('uk', '259.31')]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label   1    2    3   4   5\n",
      "topic                      \n",
      "0       0  102    0   0   0\n",
      "1       0    0    0   0  82\n",
      "2       0    0  113   0   1\n",
      "3      90    0    0   0   4\n",
      "4       6    5    2  88   7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.96      0.94      0.95        96\n",
      "           2       1.00      0.95      0.98       107\n",
      "           3       0.99      0.98      0.99       115\n",
      "           4       0.81      1.00      0.90        88\n",
      "           5       1.00      0.87      0.93        94\n",
      "\n",
      "    accuracy                           0.95       500\n",
      "   macro avg       0.95      0.95      0.95       500\n",
      "weighted avg       0.96      0.95      0.95       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Q1\n",
    "    \n",
    "    print(\"Test Q1\")\n",
    "   \n",
    "    data=pd.read_csv(\"hw4_test.csv\")\n",
    "    preds = analyze_sentiment(data[\"text\"], data[\"label\"])\n",
    "    \n",
    "    print(\"\\n==================\\n\")\n",
    "    print(\"Test Q2\")\n",
    "    \n",
    "    train=pd.read_csv(\"hw5_train.csv\")\n",
    "    test=pd.read_csv(\"hw5_test.csv\")\n",
    "\n",
    "    predicted_label = analyze_topics(train[\"text\"], test[\"text\"], test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
